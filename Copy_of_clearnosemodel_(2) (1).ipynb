{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYA6RgFX0uu8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define function to perform clustering and plot results\n",
        "def perform_clustering(csv_file_path, num_clusters=3):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Display the first few rows of the dataframe\n",
        "    print(\"First 5 rows of the dataset:\\n\", df.head())\n",
        "\n",
        "    # Check if there are any missing values\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        print(\"The dataset contains missing values. Filling missing values with the mean.\")\n",
        "        df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    # Standardize the data (since clustering can be sensitive to different feature scales)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "\n",
        "    # Add cluster labels to the original dataset\n",
        "    df['Cluster'] = kmeans.labels_\n",
        "\n",
        "    # Plotting the clusters (for the first two features, assuming numerical data)\n",
        "    plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=kmeans.labels_, cmap='viridis')\n",
        "    plt.title('Clustering results')\n",
        "    plt.xlabel('Feature 1 (Standardized)')\n",
        "    plt.ylabel('Feature 2 (Standardized)')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.show()\n",
        "\n",
        "    # Return the dataframe with the cluster labels\n",
        "    return df"
      ],
      "metadata": {
        "id": "y-6BmhqBCPEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_clustering(csv_file_path, num_clusters=3):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Display the first few rows of the dataframe\n",
        "    print(\"First 5 rows of the dataset:\\n\", df.head())\n",
        "\n",
        "    # Select only numeric columns\n",
        "    df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "    # Check if there are any missing values in numeric data\n",
        "    if df_numeric.isnull().sum().sum() > 0:\n",
        "        print(\"The dataset contains missing values. Filling missing values with the mean.\")\n",
        "        df_numeric.fillna(df_numeric.mean(), inplace=True)\n",
        "\n",
        "    # Standardize the data (since clustering can be sensitive to different feature scales)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df_numeric)\n",
        "\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "\n",
        "    # Add cluster labels to the original dataset (df_numeric)\n",
        "    df_numeric['Cluster'] = kmeans.labels_\n",
        "\n",
        "    # Plotting the clusters (for the first two features)\n",
        "    plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=kmeans.labels_, cmap='viridis')\n",
        "    plt.title('Clustering results')\n",
        "    plt.xlabel('Feature 1 (Standardized)')\n",
        "    plt.ylabel('Feature 2 (Standardized)')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.show()\n",
        "\n",
        "    # Return the dataframe with the cluster labels\n",
        "    return df_numeric\n"
      ],
      "metadata": {
        "id": "oVDinO3ACpPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn matplotlib\n"
      ],
      "metadata": {
        "id": "wwseyiibBAlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_clusters = perform_clustering('clearnose6.csv', num_clusters=4)\n"
      ],
      "metadata": {
        "id": "piBfJNpsC7IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Q8P16p1rJJg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame with cluster labels to a new CSV file\n",
        "output_csv_file = 'clustered_output.csv'\n",
        "df_with_clusters.to_csv(output_csv_file, index=False)\n",
        "print(f\"The clustered data has been saved to {output_csv_file}.\")\n"
      ],
      "metadata": {
        "id": "_fUX6qzWD7qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_clustering(csv_file_path, num_clusters=3):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Display the first few rows of the dataframe\n",
        "    print(\"First 5 rows of the dataset:\\n\", df.head())\n",
        "\n",
        "    # Clean the Price column\n",
        "    df['Price'] = df['Price'].str.extract('(\\d+)').astype(float)\n",
        "\n",
        "    # Select only numeric columns\n",
        "    df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "    # Check if there are any missing values in numeric data\n",
        "    if df_numeric.isnull().sum().sum() > 0:\n",
        "        print(\"The dataset contains missing values. Filling missing values with the mean.\")\n",
        "        df_numeric.fillna(df_numeric.mean(), inplace=True)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df_numeric)\n",
        "\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "\n",
        "    # Add cluster labels to the original dataset (df_numeric)\n",
        "    df_numeric['Cluster'] = kmeans.labels_\n",
        "\n",
        "    # Plotting the clusters\n",
        "    plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=kmeans.labels_, cmap='viridis')\n",
        "    plt.title('Clustering results')\n",
        "    plt.xlabel('Feature 1 (Standardized)')\n",
        "    plt.ylabel('Feature 2 (Standardized)')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.show()\n",
        "\n",
        "    # Return the dataframe with the cluster labels and the scaled data\n",
        "    return df_numeric, scaled_data\n"
      ],
      "metadata": {
        "id": "fcahCIpjEZkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
        "\n",
        "def perform_clustering(csv_file_path, num_clusters=3):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    print(\"First 5 rows of the dataset:\\n\", df.head())\n",
        "\n",
        "    # Clean the Price column\n",
        "    df['Price'] = df['Price'].astype(str)\n",
        "\n",
        "    # Extract numeric values from the Price column\n",
        "    df['Price'] = df['Price'].str.extract('(\\d+)').astype(float)\n",
        "\n",
        "    # Select only numeric columns (Discount, Reviews_rate, Price)\n",
        "    df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "    # Check if there are any missing values in numeric data\n",
        "    if df_numeric.isnull().sum().sum() > 0:\n",
        "        print(\"The dataset contains missing values. Filling missing values with the mean.\")\n",
        "        df_numeric.fillna(df_numeric.mean(), inplace=True)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df_numeric)\n",
        "\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "\n",
        "    # Add cluster labels to the original dataset (df_numeric)\n",
        "    df_numeric['Cluster'] = kmeans.labels_\n",
        "\n",
        "    # Force a 3D Plotting using duplicated features if there are fewer than 3 features\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # If fewer than 3 features, duplicate the second feature for 3D plotting\n",
        "    if scaled_data.shape[1] < 3:\n",
        "        ax.scatter(scaled_data[:, 0], scaled_data[:, 1], scaled_data[:, 1],  # Duplicated the second feature\n",
        "                   c=kmeans.labels_, cmap='viridis', s=50)\n",
        "        ax.set_zlabel('Feature 2 (Duplicated)')\n",
        "    else:\n",
        "        # Plot the actual first three features if available\n",
        "        ax.scatter(scaled_data[:, 0], scaled_data[:, 1], scaled_data[:, 2],\n",
        "                   c=kmeans.labels_, cmap='viridis', s=50)\n",
        "        ax.set_zlabel('Feature 3 (Standardized)')\n",
        "\n",
        "    ax.set_title('3D Clustering results')\n",
        "    ax.set_xlabel('Feature 1 (Standardized)')\n",
        "    ax.set_ylabel('Feature 2 (Standardized)')\n",
        "\n",
        "    plt.colorbar(ax.scatter(scaled_data[:, 0], scaled_data[:, 1], scaled_data[:, 1],\n",
        "                            c=kmeans.labels_, cmap='viridis'))\n",
        "    plt.show()\n",
        "\n",
        "    # Return the dataframe with the cluster labels and the scaled data\n",
        "    return df_numeric, scaled_data"
      ],
      "metadata": {
        "id": "dbBU1SHVE4CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_clusters, scaled_data = perform_clustering('clearnose6.csv', num_clusters=6)"
      ],
      "metadata": {
        "id": "Uo-nfXGhEdST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Suppose num_clusters was used in your perform_clustering function.\n",
        "num_clusters = 4  # If not defined earlier, define the number of clusters here.\n",
        "\n",
        "# Calculate silhouette score\n",
        "sil_score = silhouette_score(scaled_data, df_with_clusters['Cluster'])\n",
        "print(f'Silhouette Score for {num_clusters} clusters: {sil_score:.2f}')\n"
      ],
      "metadata": {
        "id": "QVH2WZV2EBVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# โหลดข้อมูลจาก CSV\n",
        "df = pd.read_csv('clearnose6.csv')\n"
      ],
      "metadata": {
        "id": "y_U3rzF25BJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TGqtVEhZlvdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('clearnose6.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "C4sTRhD2Ku_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CSV and columns\n",
        "\n",
        "\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# label_encoder object knows\n",
        "# how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "df['Product_names']= label_encoder.fit_transform(df['Product_names'])\n"
      ],
      "metadata": {
        "id": "HDikUdujN2g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna()"
      ],
      "metadata": {
        "id": "iQ7szOg1N3u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.iloc[12:14]\n",
        "df2"
      ],
      "metadata": {
        "id": "X2OkOgNnoawy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df.iloc[0:11]"
      ],
      "metadata": {
        "id": "4rOE_ZIIoMIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.concat([df1,df2])"
      ],
      "metadata": {
        "id": "8TmAFNZFom42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df3['Price']\n",
        "X = df3.drop(columns=['Price','Product_names'])"
      ],
      "metadata": {
        "id": "_WXD7gcDoBZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "PivY1sl5oDb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "# plt.plot(X_test, regr.predict(X_test), color='red',linewidth=3)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "O1B763IhMRwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "zKDiXynIO4Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Make predictions using the test set\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "# Compute R-squared score\n",
        "r2_score = metrics.r2_score(y_test, y_pred)\n",
        "print(f'R-squared score: {r2_score}')\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "mse = metrics.mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n"
      ],
      "metadata": {
        "id": "bP5vRv3PO0Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming 'model' is your trained model\n",
        "filename = 'model.pkl'\n",
        "\n",
        "# Save the model to disk\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(regr, file)"
      ],
      "metadata": {
        "id": "mO82ExK1PyZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}